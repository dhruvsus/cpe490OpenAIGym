{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Reinforcement Learning to Cartpole\n",
    "by Thomas Fahrner, Mikey Maramba, and Dhruv Singal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is reinforcement learning?\n",
    "Recent progress in deep learning has expanded beyond unsupervised and supervised learning. Reinforcement learning (RL) is a class of algorithms that establish an action-reward framework to develop an agent that chooses actions to maximize rewards [1]. This framework lends itself to Markov decision processes (MDP) [2]. The RL field has flocked to video games since they can often be analyzed as MDPs. OpenAI has created a \"gym\" where users can download environments (games) [3]. Users then train models on these environments in an attempt to converge on a high score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole\n",
    "Cartpole is an inverted pendulum environment from OpenAI [4]. The goal of Cartpole is to balance the pole in an upright position. Agents have the ability to apply a left or right force on the cart. After each action/timestep, the environment returns an updated state and a reward. The game ends when the pole falls beyond 15 degrees from vertical or the cart extends beyond 2.4 units from the center. To \"win\" the game, the agent must receive an average reward of 195 over 100 trials. Each \"episode\" ends when either the game ends or the maximum reward (set by default to 200) is achieved.\n",
    "\n",
    "The state has four features: [position of cart, velocity of cart, angle of pole, rotation rate of pole] = [p, v, a, r]\n",
    "\n",
    "The reward is +1 after every timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Cartpole/OpenAI Gym Environment\n",
    "After installing OpenAI Gym via pip, you can use the core environments that gym provides. The OpenAI Gym Environment provides users with environments and functions used to extract data from the environments for training. The central function is the step function, which returns 4 values: observation, reward, done, and info. Observation stores the 'state' of the game, reward stores the amount of reward achieved by the previous action, and done determines whether to reset the environment or not (info is mostly diagnostic information).\n",
    "\n",
    "Creating the Cartpole environment is as simple as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the function test_params to test the parameter sets that we generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params(env, params):\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        #env.render()\n",
    "        res = np.matmul(params, observation)\n",
    "        if res < 0:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #print(observation)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the functions we will use to display the results of our algorithms. We will utilize various NumPy utilities and functions to perform matrix operations and use matplotlib to visualize the results of our algorithms' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "env._max_episode_steps = 200  # Cap CartPole maximum reward to this amount\n",
    "num_samples = 10  # Number of samples to utilize for each algorithm\n",
    "\n",
    "\n",
    "def runHillClimbing(env):\n",
    "    num_episodes = []\n",
    "    for i in range(num_samples):\n",
    "        res = climb_hill(env)\n",
    "        if res:\n",
    "            num_episodes.append(res)\n",
    "\n",
    "    plt.hist(num_episodes)\n",
    "    plt.title('Hill Climbing Algorithm')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "def runGuessing(env):\n",
    "    num_episodes = []\n",
    "    for i in range(num_samples):\n",
    "        num_episodes.append(run_guesses(env))\n",
    "\n",
    "    plt.hist(num_episodes)\n",
    "    plt.title('Random Guessing Algorithm')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "def runPolicyGradient(env):\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_outputs = env.action_space.n\n",
    "    model = build_network(num_inputs, num_outputs)\n",
    "    train = build_train(model, num_outputs)\n",
    "\n",
    "    steps_per_sample = []\n",
    "    for sample in range(num_samples):\n",
    "        for episode in range(5000):\n",
    "            reward = run_episode(env, train, model, num_outputs)\n",
    "            if reward == env._max_episode_steps:\n",
    "                steps_per_sample.append(episode)\n",
    "                break\n",
    "        reset_model(model)\n",
    "\n",
    "    plt.hist(steps_per_sample)\n",
    "    plt.title('Policy Gradient Algorithm')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Approach:\n",
    "We developed a variety of different algorithms to solve Cartpole. Our goals were twofold: gain exposure to the basics of RL and incorporate the deep learning techniques we learned in 490.\n",
    "\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Parameters \n",
    "This algorithm involves creating up to N sets of parameters randomly (for our environment, a size of 10,000 parameter sets would be more than sufficient). Each set of parameters were of length 4 and set between -1 and 1. The parameter set was then tested until the game was over. If the current set of parameters achieved a greater amount of reward than the previous maximum, the set would be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_guesses(env):\n",
    "    best_params = None\n",
    "    best_reward = 0\n",
    "    t_found = None\n",
    "\n",
    "    for t in range(10000):  # 10,000 random configurations, 1 each episode\n",
    "        params = np.random.rand(4) * 2 - 1  # Random weights btwn -1 and 1\n",
    "        reward = test_params(env, params)\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_params = params\n",
    "            t_found = t+1\n",
    "            if reward == env._max_episode_steps:\n",
    "                break\n",
    "\n",
    "    return t_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runGuessing(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climbing\n",
    "This algorithm involves creating a set of parameters randomly (similar to the random parameters algorithm), testing our initial set of parameters, and then adding “noise” to the parameter set. This noise would be a random set of four numbers between -1 and 1, scaled by a factor of our choice. Similar to the other model, if the current set of parameters achieved a greater amount of reward than the previous maximum, the set would be saved. Noise would be added up to N times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climb_hill(env, scale_noise=0.1):\n",
    "    best_params = None\n",
    "    best_reward = 0\n",
    "    t_found = None\n",
    "\n",
    "    # Start with random setting of the parameters\n",
    "    params = np.random.rand(4) * 2 - 1\n",
    "\n",
    "    for t in range(10000):  # Number of iterations to \"climb\"\n",
    "        noise = (np.random.rand(4) * 2 - 1) * scale_noise\n",
    "        params = params + noise  # Add noise matrix to params\n",
    "        reward = test_params(env, params)\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_params = params\n",
    "            t_found = t+1\n",
    "            if reward == env._max_episode_steps:\n",
    "                break\n",
    "\n",
    "    if best_reward == env._max_episode_steps:\n",
    "        return t_found\n",
    "    else:\n",
    "        return None  # return 0 if we did not find desired reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runHillClimbing(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "We adopted a stochastic policy that defines a probability for each action given the current state. We created a loss and training function to influence the probabilities with the rewards. Our training function considers the difference between the reward seen at a state and the average reward. An action with a reward much larger than the average will have its probability increased for that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "from keras import optimizers\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def build_network(num_inputs, num_outputs):\n",
    "    # use Keras functional API to construct policy network\n",
    "    network = layers.Input(shape=(num_inputs,))\n",
    "    input = network\n",
    "\n",
    "    for layer in [32, 16, 8]:\n",
    "        network = layers.Dense(layer)(network)\n",
    "        network = layers.Activation(\"relu\")(network)\n",
    "\n",
    "    network = layers.Dense(num_outputs)(network)\n",
    "    network = layers.Activation(\"softmax\")(network)\n",
    "\n",
    "    return Model(inputs=input, outputs=network)\n",
    "\n",
    "\n",
    "def build_train(model, num_outputs):\n",
    "    prob_action = model.output\n",
    "    encoded_action = K.placeholder(shape=(None, num_outputs), name=\"encoded_action\")\n",
    "    discount_reward = K.placeholder(shape=(None,), name=\"discount_reward\")\n",
    "\n",
    "    loss = K.mean(-K.log(K.sum(prob_action * encoded_action, axis=1)) * discount_reward)\n",
    "\n",
    "    deltas = optimizers.Adam().get_updates(params=model.trainable_weights, loss=loss)\n",
    "\n",
    "    return K.function(inputs=[model.input, encoded_action, discount_reward], outputs=[], updates=deltas)\n",
    "\n",
    "\n",
    "def get_action(model, state, num_outputs):\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    prob_action = np.squeeze(model.predict(state))\n",
    "\n",
    "    return np.random.choice(np.arange(num_outputs), p=prob_action)\n",
    "\n",
    "\n",
    "def fit(train, states, actions, rewards, num_outputs):\n",
    "    encoded_action = np_utils.to_categorical(actions, num_classes=num_outputs)\n",
    "    discount_reward = get_discounted(rewards)\n",
    "    train([states, encoded_action, discount_reward])\n",
    "\n",
    "\n",
    "def get_discounted(rewards):\n",
    "    discount_rate = 0.96\n",
    "    discounted = np.zeros_like(rewards, dtype=np.float32)\n",
    "    sum = 0\n",
    "\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        sum = sum * discount_rate + rewards[t]\n",
    "        discounted[t] = sum \n",
    "\n",
    "    discounted -= discounted.mean() / discounted.std()\n",
    "\n",
    "    return discounted\n",
    "\n",
    "\n",
    "def run_episode(env, train, model, num_outputs):\n",
    "    is_done = False\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    total_reward = 0\n",
    "\n",
    "    while not is_done:\n",
    "        action = get_action(model, state, num_outputs)\n",
    "        new_state, reward, is_done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    fit(train, np.asarray(states), np.asarray(actions), np.asarray(rewards), num_outputs)\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def reset_model(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runPolicyGradient(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning\n",
    "The set of states and actions, together with rules for transitioning from one state to another, make up a Markov decision process. One episode of this process (e.g. one game) forms a finite sequence of states, actions and rewards:\n",
    "A Markov decision process relies on the Markov assumption, that the probability of the next state si+1 depends only on current state si and action ai, but not on preceding states or actions.\n",
    "![title](./MarkovDecision.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that, the total future reward from time point t onward can be expressed as:\n",
    "![title](./Rt.png)\n",
    "Here γ is the discount factor between 0 and 1 – the more into the future the reward is, the less we take it into consideration. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./policy.png)\n",
    "Where Q(s, a) represents the maximum discounted future reward when we perform action a in state s, and continue optimally from that point on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./bellman.png)\n",
    "This is called the Bellman equation. The main idea in Q-learning is that we can iteratively approximate the Q-function using the Bellman equation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "![title](./alg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "\n",
    "class DQNetwork:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        # state inputs to the Q-network\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Dense(10, activation='relu', input_dim=4))\n",
    "        self.model.add(Dense(10, activation='relu'))\n",
    "        self.model.add(Dense(2, activation='linear'))\n",
    "        self.optimizer = 'adagrad'\n",
    "        self.model.compile(loss='mse', optimizer=self.optimizer)\n",
    "\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.b = deque(maxlen=max_size)\n",
    "\n",
    "    def learn(self, exp):\n",
    "        self.b.append(exp)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(\n",
    "            np.arange(len(self.b)), size=batch_size, replace=False)\n",
    "        return [self.b[i] for i in idx]\n",
    "\n",
    "\n",
    "train_episodes = 500  # max number of episodes to learn from\n",
    "max_steps = 200  # max steps in an episode\n",
    "gamma = 0.99  # future reward discount\n",
    "batch_size = 32  # experience mini-batch size\n",
    "pretrain_length = batch_size  # number experiences to pretrain the memory\n",
    "\n",
    "mainQN = DQNetwork()\n",
    "#resest environment before starting to build memory\n",
    "env.reset()\n",
    "# do something random to start environment\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "state = np.reshape(state, [1, 4])\n",
    "memory = Memory()\n",
    "\n",
    "for ii in range(pretrain_length):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, 4])\n",
    "\n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        memory.learn((state, action, reward, next_state))\n",
    "\n",
    "        env.reset()\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "        state = np.reshape(state, [1, 4])\n",
    "    else:\n",
    "        memory.learn((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "step = 0\n",
    "steps_per_sample=[]\n",
    "# starting episodes from 1\n",
    "for ep in range(1, train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    while t < max_steps:\n",
    "        step += 1\n",
    "        # env.render()\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            # the episode ends so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            steps_per_sample.append(t)\n",
    "            t = max_steps\n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Total reward: {}'.format(total_reward))\n",
    "            # Add experience to memory\n",
    "            memory.learn((state, action, reward, next_state))\n",
    "            # Start new episode\n",
    "            env.reset()\n",
    "            # Take one random step to get the pole and cart moving\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            state = np.reshape(state, [1, 4])\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.learn((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "\n",
    "        # Replay\n",
    "        inputs = np.zeros((batch_size, 4))\n",
    "        targets = np.zeros((batch_size, 2))\n",
    "\n",
    "        minibatch = memory.sample(batch_size)\n",
    "        for i, (state_b, action_b, reward_b,\n",
    "                next_state_b) in enumerate(minibatch):\n",
    "            inputs[i:i + 1] = state_b\n",
    "            target = reward_b\n",
    "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):\n",
    "                target_Q = mainQN.model.predict(next_state_b)[0]\n",
    "                target = reward_b + gamma * np.amax(\n",
    "                    mainQN.model.predict(next_state_b)[0])\n",
    "            targets[i] = mainQN.model.predict(state_b)\n",
    "            targets[i][action_b] = target\n",
    "        mainQN.model.fit(inputs, targets, epochs=1, verbose=0)\n",
    " #print(steps_per_sample)\n",
    "plt.hist(steps_per_sample)\n",
    "plt.title('DQN Algorithm')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('dqn.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./dqn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Since there are only 4 parameters involved in the Cartpole environment, it is not surprising to see simple algorithms such as Random Parameters consistently outperforming the more computationally intensive algorithms such as Policy Gradient. For environments with higher dimensionality involved in the parameter sets, we would expect to see the more complex algorithms outperform the simple algorithms. \n",
    "\n",
    "Another possibility we would like to extend in the future is the use of convolutional neural networks to base our decision making on the rendered images instead of basing our decisions on the game's state via the observation property of the environment's step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://ai.intel.com/demystifying-deep-reinforcement-learning/ \n",
    "2. https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b\n",
    "3. https://gym.openai.com/\n",
    "4. https://gym.openai.com/envs/CartPole-v0/\n",
    "5. http://www.scholarpedia.org/article/Policy_gradient_methods\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
